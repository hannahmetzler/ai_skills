---
format: 
  revealjs: 
    transition: "slide"
    ## Defines the theme of the presentation, both version and style
    theme: [default, custom.scss]
    # incremental slides point by point
    incremental: false
    aspect-ratio: 16:9
    slide-number: true
    speakerNotes: true
# title-slide-attributes:
# data-background-image: images/
# data-background-size: cover
# data-background-opacity: "0.5"
#editor
editor: source
---


## Introduction to LLMs & Important Ethical Aspects for Everyday Use in Research

### Mastering AI Chatbots: Optimizing Research through Effective Use

<br>

**Mag. Dr. Hannah Metzler** <br>
*Complexity Science Hub &  Medical University of Vienna* 

<br>

Slides: https://hannahmetzler.eu/ai_skills

## AI and Science

::::: columns
::: {.column width="47%"}
![](../Introduction/images/ai_scientist_news.png)
<small>*Source: [Discover Magazine](https://www.discovermagazine.com/the-sciences/meet-the-ai-scientist)*</small>
:::

::: {.column width="47%" .fragment}
![](../Introduction/images/comparing_numbers.png)

:::
:::::

## What is an LLM, actually? 

- **Large Language Models (LLMs)**: Deep learning neural networks built to understand and generate text
- Trained on **vast amounts of text** from diverse sources (Internet, Wikipedia, social media, books, podcasts, etc.)
- Convert text data into **statistical patterns** to predict words and sentences


## How are LLMs trained?

**GPT**:
**G**enerative **P**re-trained **T**ransformer models (a model architecture for efficient computing):

- Generative: Produces/Generates Text
- Pre-trained: on large text corpus.
  - Autoregressive:
      - Always predicts the next word of a text.
      - Does not need any annotations by humans. 
- Transformer: type of the language model

## Next word prediction

![](../Introduction/images/next_word_prediction.png)

## Text representation
<br>
<br>

::::: columns
::: {.column width="40%" align="center"}
Words

<small>
Even though the sound of it is<br> something quite atrocious, if you<br> say it loud enough, you’ll always<br> sound precocious, <br>Supercalifragilisticexpialidocious!
</small>

<small>
*Marry Poppins*
</small>

:::

::: {.column width="10%" align="center".fragment data-fragment-index="1"}
→
:::

::: {.column width="40%" align="center" .fragment data-fragment-index="1"}
Tokens = "Subwords"

![](../Introduction/images/tokenzined_sentence.png)
:::

:::::

::: notes
- Dictionary with specific number of tokens
- constructs the rare words by combining tokens
- Every single character is also one token, so that the LLM can construct all possible words from tokens.
:::

## Each token is a vector of numbers (= a word embedding)
<br>
<br>

::::: columns
::: {.column width="40%" align="center"}
Word/Token


<br>

<span
style="padding: 0.1em 0.em; background-color: #D7C9F3; border-radius: 4px;">Even
</span>
...

:::

::: {.column width="10%" align="center" .fragment data-fragment-index="1"}
→
:::
::: {.column width="40%" align="center" .fragment data-fragment-index="1"}
Word Embedding
<small>
[-0.3185, 0.5976, 0.4817, 0.7306, -0.5938, -0.6372, 0.9381, -0.9165, -0.9396, 0.3540,
0.0262, -0.6131, 0.3634, -0.0391, -0.4732, -0.2341, -0.8044, -0.3637, -0.5958, -0.8710,
0.3722, -0.8544, -0.7819, -0.5487, -0.9314, 0.3949, -0.3168, -0.3363, -0.6973, -0.3789,
0.7200, -0.6201, -0.7010, -0.3735, 0.7437, -0.9795, -0.4916, 0.2130, 0.6817, 0.1972,
0.8518, -0.8700, -0.4013, -0.6310, -0.9597, 0.2763, -0.9173, 0.2900, -0.1896, 0.8286,
-0.8617, 0.2566, 0.7024, -0.2448, 0.0994, -0.6664, -0.0699, -0.5830]
</small>
:::
:::::


## Representing the meaning of words

::::: columns
::: {.column width="40%"}
**Word embeddings**

![](../Introduction/images/wordembeddings.svg)

:::

::: {.column width="56%"}
**Contextual word embeddings**

-   Context (before and after the word) taken into account
-   [Example](https://www.tiktok.com/@whats_ai/video/7247609946672565510) "Apple": Mac or fruit?

<img src="../Introduction/images/apple_context.png" alt="Apple context" style="display: block; margin-left: auto; margin-right: auto; width: 250px;" />
:::
:::::

## In technical words

-   Deep Learning: Multiple layers of "neurons"
-   Linear regressions calculate "weights" for the connections

::::: columns
::: {.column width="47%"}
-   Input layer: Quantitative representation of words
-   Output layer: One-hot encoding
    - One (of the) word(s) with high probability gets a 1, all others 0. 
:::

::: {.column width="47%"}
```{r, echo=FALSE}
knitr::include_graphics("../Introduction/images/neuralnet.svg")
```
:::
:::::

::: notes
One hot encoding: output has as many units as words, we predict the one with the maximum value, probabilities of words Input: one-hot times embedding, everything but the mask
:::



<!-- ## Steps for training of frontier LLMs -->

<!-- 1. Pre-train on most of the available data from the internet via next-word prediction. -->
<!-- 1. Fine-tune model on conversation setting and chain-of-thought reasoning (next slide). -->
<!-- 1. Refine with feedback from humans -->
<!-- 1. In context learning (Prompting): Explain the task in the context window. -->


## What is the model still missing?

![](../Introduction/images/unknown_caller.png)

. . . 

<br>

**Context information: prompting to give more context**

:::footer
[pixabay](https://pixabay.com/illustrations/keyboard-smartphone-unknown-call-6753584/)
:::

## Current top models

-   **Gemini-Exp & Gemine-1.5 Pro** (Google)
-   **ChatGPT-4o and o1** (Open AI)
-   **Claude 3.5 Sonnet** (Anthropic)


best non-proprietary:
-   **Llama 3.1** (Meta)

:::footer
[Chatbot Arena](https://chat.lmsys.org/)
:::

## Strengths of different models and interfaces

-   [**ChatGPT-4o**](https://chat.openai.com/) (Open AI): web search, code execution, multimodal interactions, custom GPTs with document upload, context: 128.000 tokens.
-   [**Claude 3.5 Sonnet**](https://claude.ai/) (Anthropic): Writing style, beautiful text rendering, custom projects, context: 200.000 tokens
-   [**Gemini 1.5 pro**](https://gemini.google.com/app) (Google): web search, code execution, multimodal interactions, integration with Google services, custom models, context: up to 2 million tokens


::: notes
multimodal interactions with ChatGPT described on next slide
:::


## Multimodal interactions

::::: columns
::: {.column width="48%"}
**ChatGPT & Gemini**

- Browse
- Create images
- Execute code
- Voice input/output (Claude: input with extensions)
- Discovering & using custom GPTs/Gems (Claude: Projects)
:::

::: {.column width="48%"}
**All models**

- File Uploads
- Upload & analyze images
- Data analysis

**Only Claude**

- Interactive artifacts
- Format text & websites
:::

:::::

::: footer
This changes constantly!
:::

:::notes
Artifacts allows for creating standalone, interactive content directly within the chat interface. Can be created and modified in real-time.  Interactive web components, dashboards interactive tables, data visualizations, svg graphics etc. 
:::

## Different ChatGPT versions {.smaller}

- GPT4: more resource intensive = expensive
- GPT4o: optimized to be faster and cheaper
- GPT-4o mini: every day tasks
- o1-preview: thinks before it answers, for complex tasks (no web search)
  
  
$\rightarrow$ Konstantin's default: GPT4


Usage limits:
- GPT-4: 40 messages every 3 hours
- GPT-4o: 80 messages every 3 hours
- GPT-4o mini: no limit

- o1-preview: 50 messages per week 
- o1-mini: 50 messages a day 

::: footer
[ChatGPT blog on models](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini) [usage limits](https://help.openai.com/en/articles/6950777-what-is-chatgpt-plus#h_d78bb59065)

::: 


## Appendix {visibility='hidden'}

### Access to models

| model      | free access                       | costs /month |
|------------|-----------------------------------|--------------|
| ChatGPT    | GPT-4o mini, GPT4-o limited, limits for multimodal   | 24€          |
| Claude   | 200 tokens/day                    | ca \$20      |
| Gemini      | 1 month trial of paid version, <br> high usage limits in free version                   | 22€     |
| Perplexity | 3 requests/day, 1000 tokens/month | \$17-20      |
| Elicit     | 1000 tokens/month                 | \$10-12      |
