---
title: "Introduction to Large Language Models (LLMs)"
subtitle: "Mastering AI Chatbots: Optimizing Research through Effective Use"
author: "Konstantin Hebenstreit"
date: "October 22 2024"
format: 
  revealjs: 
    transition: "slide"
    ## Defines the theme of the presentation, both version and style
    theme: [default, custom.scss]
    # incremental slides point by point
    incremental: false
    aspect-ratio: 16:9
    slide-number: true
# title-slide-attributes:
# data-background-image: images/
# data-background-size: cover
# data-background-opacity: "0.5"
#editor
editor: source
---

## AI and Science

::::: columns
::: {.column width="47%"}
![](images/ai_scientist_news.png)
<small>*Source: [Discover Magazine](https://www.discovermagazine.com/the-sciences/meet-the-ai-scientist)*</small>
:::

::: {.column width="47%" .fragment}
![](images/comparing_numbers.png)

:::
:::::

## How does AI work?
<br>
<br>

::::: columns
::: {.column #vcenter width="20%" align="center"}
Input
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="25%" align="center"}
AI Model
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="20%" align="center"}
Output
:::
:::::

## AI
<br>
<br>

::::: columns
::: {.column #vcenter width="20%" align="center"}
![](https://images.pexels.com/photos/1414110/pexels-photo-1414110.jpeg)
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="25%" align="center"}
Image Classification Model
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="20%" align="center"}
Lemon
:::
:::::

## LLMs
<br>
<br>

::::: columns
::: {.column #vcenter width="20%" align="center"}
Text
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="25%" align="center"}
Large Language Model
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="20%" align="center"}
Text
:::
:::::


## LLMs
<br>
<br>

::::: columns
::: {.column #vcenter width="15%" align="center"}
Text
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="40%" align="center"}
![](images/tunable_parameters.png)
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="15%" align="center"}
Text
:::
:::::

## LLMs: Autocomplete
<br>
<br>

::::: columns
::: {.column #vcenter width="15%" align="center"}
<small>"The actress that played Rose in the 1997 film Titanic is named..."</small>
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="40%" align="center"}
![](images/tunable_parameters.png)
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="15%" align="center"}
_Predicts the next word_
:::
:::::

## Next word prediction

![](images/next_word_prediction.png)

## Text representation
<br>
<br>

::::: columns
::: {.column width="40%" align="center"}
Words

<small>
Even though the sound of it is<br> something quite atrocious, if you<br> say it loud enough, you’ll always<br> sound precocious, <br>Supercalifragilisticexpialidocious!
</small>

<small>
*Marry Poppins*
</small>

:::

::: {.column width="10%" align="center".fragment data-fragment-index="1"}
→
:::

::: {.column width="40%" align="center" .fragment data-fragment-index="1"}
Tokens = "Subwords"

![](images/tokenzined_sentence.png)
:::

:::::

## Text representation
<br>
<br>

::::: columns
::: {.column width="40%" align="center"}
Word/Token

<!-- ![](images/tokenzined_sentence.png) -->
<br>
<span style="padding: 0.1em 0.em; background-color: #D7C9F3; border-radius: 4px;">Even</span> ...
:::

::: {.column width="10%" align="center" .fragment data-fragment-index="1"}
→
:::
::: {.column width="40%" align="center" .fragment data-fragment-index="1"}
Word Embedding
<small>
[-0.3185, 0.5976, 0.4817, 0.7306, -0.5938, -0.6372, 0.9381, -0.9165, -0.9396, 0.3540,
0.0262, -0.6131, 0.3634, -0.0391, -0.4732, -0.2341, -0.8044, -0.3637, -0.5958, -0.8710,
0.3722, -0.8544, -0.7819, -0.5487, -0.9314, 0.3949, -0.3168, -0.3363, -0.6973, -0.3789,
0.7200, -0.6201, -0.7010, -0.3735, 0.7437, -0.9795, -0.4916, 0.2130, 0.6817, 0.1972,
0.8518, -0.8700, -0.4013, -0.6310, -0.9597, 0.2763, -0.9173, 0.2900, -0.1896, 0.8286,
-0.8617, 0.2566, 0.7024, -0.2448, 0.0994, -0.6664, -0.0699, -0.5830]
</small>
:::
:::::

## Representing the meaning of words

::::: columns
::: {.column width="40%"}
**Word embeddings**

```{r}
knitr::include_graphics("images/wordembeddings.svg")
```
:::

::: {.column width="56%"}
**Contextual word embeddings**

-   Context (before and after the word) taken into account
-   [Example](https://www.tiktok.com/@whats_ai/video/7247609946672565510) "Apple": Mac or fruit?

```{r, fig.align="center", out.width=250}
knitr::include_graphics("images/apple_context.png")
```
:::
:::::

## What is an LLM, actually?

- **Large Language Models (LLMs)**: Deep learning neural networks built to understand and generate text
- Trained on **vast amounts of text** from diverse sources (Wikipedia, social media, books, podcasts, etc.)
- Convert text data into **statistical patterns** to predict words and sentences


## Pros and Cons of LLMs

### Pros
- Generates coherent, human-like text
- Vast knowledge embedded in its parameters
- Useful for content generation, summarization, and translation

### Cons
- Cannot think before responding
- Struggles with logical reasoning, often makes incorrect assumptions
- Can produce inaccurate or hallucinated information

## Thank You for Your Attention!
<br>
<br>
**Questions? I’m happy to answer!**



<!-- ## What is an LLM, actually?

-   Large Language Models: Deep Learning Neural Networks trained on text
-   Advanced Language models that process huge amounts of text
-   Trained with text data from the Internet (Wikipedia, social media, online media, books, literature, videos, films, YouTube, podcasts...)
-   Turn it into statistical word and sentence predictions
-   Can provide detailed and context-rich answers

## Neural networks

-   Multiple layers of "neurons"
-   Deep learning
-   Linear regressions calculate "weights" for the connections

::::: columns
::: {.column width="47%"}
-   Input layer: Quantitative representation of sentence
-   Output layer: One-hot encoding
:::

::: {.column width="47%"}
```{r, echo=FALSE}
knitr::include_graphics("images/neuralnet.svg")
```
:::
:::::

::: notes
One hot encoding: output has as many units as words, we predict the one with the maximum value, probabilities of words Input: one-hot times embedding, everything but the mask
:::



## How are LLMs trained? - update

All Transformer models (a model architecture for efficient computing):

-   BERT specific, to update: **Masked word prediction**
    -   “New **\[MASK\]** is a city”
-   **Next sentence prediction**
    -   "Today is a beautiful sunny day, and I decided to go for a long walk in the park. Suddenly, the quadratic equation was solved."\
    -   TRUE/FALSE als Feedback
    
## Terms to explain

- context window
- token

## Current top models - To do: update

-   **ChatGPT-4o** (Open AI)
-   **Claude** (Anthropic)
-   **Bard (Gemini)** (Google)

## Multimodal ChatGPT interactions

**(Some also work with Claude)**

- File Uploads (& Claude)
- Browse
- Discovering and using GPTs
- Upload & analyze images
- Create images with DALL-E
- Audio: Talking/Listening
- Data analysis (& Claude)
- Execute code

## Strengths of different models - update

-   [**ChatGPT-4**](https://chat.openai.com/) (Open AI): multimodal interactions, 8.000 tokens (more with API)
-   [**Claude 3.5 Sonnet**](https://claude.ai/) (Anthropic): Writing style, longer context window (200.000 tokens)
-   [**Gemini**](https://gemini.google.com/app) (Google): ??? (Search capability and database, integration with Google services), 100.000 tokens

Source: [Chatbot Arena](https://chat.lmsys.org/) - Leaderbord

## Different ChatGPT versions

- ChatGPT4: more resource intensive = expensive
- ChatGPT4o = optimized to be faster and cheaper
- ChatGPT-4o mini: every day tasks
- Only via API or special access plans:
  - ChatGPT-4 Turbo: long context window (128.000 tokens)
  - o1-preview: advanced reasoning
  - ChatGPT-3.5: less resource intense
  
::: footer
[ChatGPT blog on models](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini)

::: 

## Slide on Chatbot Arena?





## Access to models

| model      | free access                       | costs /month |
|------------|-----------------------------------|--------------|
| ChatGPT    | GPT-4o mini, GPT4-o limited, limits for multimodal   | 24€          |
| Claude   | 200 tokens/day                    | ca \$20      |
| Gemini      | 1 month trial of paid version, 2500 tokens/day                   | 21.99€     |
| Perplexity | 3 requests/day, 1000 tokens/month | \$17-20      |
| Elicit     | 1000 tokens/month                 | \$10-12      |

::: notes
To Do: update requests/limits
:::


## Discussion 

- In small groups: 


# The basics: How to talk to an LLM Chatbot?

## Use search engines vs. LLMs

:::: columns
::: {.column width="35%"}
### Search engine

-   A few short keywords
<br><br><br><br><br>
-   Answer: many single results
:::

::: {.column width="60%"}
### LLM Chatbot

-   Long prompts with lots of details, context, examples & explanations
-   Answer is summarised
:::
::::

## General tips

- Use it often and try it out!
- Speak to it as if it was a person
- Give feedback & engage in dialogue, performance improves very quickly
- Spelling doesn't matter at all
- Do the first 80% quickly, then check.
- Use microphone & headphones and actually talk!
- Work on your prompts!

## Voice Control

- ChatGPT App on Mac & Smartphones
- Otherwise: Chrome extension [Voice Up](https://voicecontrol.chat/) (ChatGPT & Claude)

:::::: columns
::: {.column width="47%"}
### Voice Up

- Hold SPACE (outside text input) to record, release to submit
- ESC to stop & transcribe text without submitting
:::

::: {.column width="47%"}
### ChatGPT App
![](images/voice_control.png)

- Headphones for an ongoing conversation
- Microphone to record, stop to transcribe
:::
::::::

## Effective Prompting

- Loooooooooooooong
- Provide context, examples

## Hallucinations

-   LLMs invent data that doesn't exist
    -   Why? Statistical language prediction
-   Getting better, but still happens

------------------------------------------------------------------------

## Strategies for verification

-   4-eyes principle: Model does 80%, 20% human
-   Follow-up questions: Check this again, is it correct? Can you give me some sources? Are you sure?
-   Enter the same prompt in different models and compare, e.g. at <https://poe.com>, <https://anakin.ai>
-   Enter the answer of the 1st model in another model & ask:
    -   Is this correct? Can you please back this up with studies?

# Applications

## Ideas for everyday life

-   Recipes\*\*: Suggestions based on existing ingredients ([example](https://chat.openai.com/c/396bfb4b-d0c3-4cd7-9239-a9076d85b374))
    -   Customise ([example less effort](https://chat.openai.com/c/8de5a079-31df-4374-b3ec-15ffe18a6bde))
-   **Travel planning**: Individual recommendations for city trips
-   Shopping lists\*\*: Schnitzels and goulash for 4 people ([example weekly meal plan](https://chat.openai.com/c/19cd2fa5-ea51-49cb-8eff-559149200170))
-   **Household**: Stain removal tips ([example](https://chat.openai.com/c/8cbd83d0-eb44-40ad-88bf-c1f227a44df0))
-   **Geschenksideen** ([Beispiel](https://chat.openai.com/c/eac1d677-9496-40b4-9f52-716bf1a00d2b))
-   Mindfulness meditation guide

------------------------------------------------------------------------

## Ideas for everyday life 2

-   **Summarise long documents**: Upload link or file
    -   Instructions from camera, microwave, light panels etc.
    -   Summarise the law ([example laying hens](https://chat.openai.com/c/17233daf-0296-4cad-befb-f0466dfb9375))
    -   Summarise long newspaper articles: listen as a podcast
-   Coaching for important career decisions
-   Recommendations for jobs/positions/roles based on my CV
-   Play city, country river

------------------------------------------------------------------------

## Ideas for academic work

-   Suggest outline/structure for a presentation
-   Feedback on presentations: Upload slide(s)
-   Overview of new topic area & brainstorming
-   Texts: revise, summarise, shorten
-   Write first draft of a text based on notes
-   Write in your own style: Upload paper
-   Surveys, e.g. develop & test MC questions, feedback on comprehensibility

------------------------------------------------------------------------

## At work: ideas

-   Summarise & record workshop & discussions: Run mic
-   Programming: LLMs are data scientists!
    -   Find errors, basic code structure...
    -   Upload data set + related questions
-   Upload long reports/papers, have them analysed
    -   e.g. compare 2 university annual reports, create a table of differences
-   Have transcripts of interviews created

## In the work: Ideas 2

-   Translation [www.deepl.com](www.deepl.com)
-   Productivity coach: screenshot of a diary for a week, or several weeks

# Outdated tipps?

## Emotional prompts

-   Emotional expressions can lead to better results ([Cheng et al., 2023](https://arxiv.org/abs/2307.11760))

```{r}
knitr::include_graphics("images/emotional_prompt.jpg")
```

```         
Write your answer and give me a confidence score between 0-1 for your answer. 
You'd better be sure.
```

------------------------------------------------------------------------

### More examples of emotional prompts:

-   This is extremely important to me, please be accurate.
-   I'll tip you \$200 if you do a good job.
-   I have no fingers, please help me.
-   If you work correctly and well, you'll get a treat.

## Why do emotional prompts work?

-   Training the model on human language, describing human interactions

-   Is a person's response likely to be better or worse if I tell whom it is important?

-   Can increase care/accuracy of the answer

-   Therefore: Talk like a human being

# Tips for advanced users

## Base or system prompts

-   After a successful interaction, format a base prompt in JSON for future requests:
-   "Please write me a megaprompt so I can do this again next time in JSON"

### Example:

Could you please write a mega prompt from our interaction here, that I can use again next time I want to ask you about coding, when I want a succinct answer? Please write it in JSON.

------------------------------------------------------------------------

### Base prompt example - Chat GPT response:

{ "request": "Please provide a concise, code-focused answer without basic instructions like how to sign in or navigate interfaces. Assume familiarity with basic concepts and procedures, and focus directly on the commands or steps needed to accomplish the task. In the code, add comments for different code blocks.",

"context": "When asking for coding or technical guidance, I prefer succinct responses that go straight to the point, providing only the essential commands or steps without additional explanations or preliminary steps assumed to be known." }

## Customizing the default version of ChatGPT

-   System prompts\*\*: Prompts that are entered each time, e.g. emotional prompts

:::::: columns
::: {.column width="15%"}
```{r}
knitr::include_graphics("images/how_to_system_prompt.png")
```
:::

::: {.column width="30%"}
```{r}
knitr::include_graphics("images/system_prompt.png")
```
:::

::: {.column width="45%"}
"This is extremely important for my job, please work accurately and check that everything is correct."
:::
::::::

::: notes
My custom Instructions: ask Konstantin if useful. 
I am based in Vienna, and am a postdoc researcher in computational social science and psychology. I am a women, I use ChatGPT to code in R. I also use Github. I check scientific information for work, my research topics include social media, misinformation, polarization, false polarization, subjective well-being, analysing social media data, suicide prevention via the media (media effects research), machine learning applications in these areas, social dominance and power, social connection and belonging, hatespeech, hormones, power postures. I do acroyoga, am part of a co-housing project, am in a relationship. I like listening to podcasts, reading books, biking, talking about relationships, meeting friends, acroyoga etc. I also supervise PhD and master students.

In my personal life, I am polyamorous, vegetarian to vegan, like effective altruism, S+, good at relationships, moving to a co housing project in 2 years, 37 years old.

Open question: How would you like ChatGPT to respond?
Think step by step. Don't give detailed explanations of code unless I ask for it. 
:::


## Creating Custom GPTs

-   Creation of a customised model for specific requirements
-   Examples:
    -   Writing in my writing style
    -   Concise answers for code, adapted to my skills

## Creating Custom GPTs: How to

:::: columns
::: {.column width="18%"}
1)  

```{r, echo=F}
knitr::include_graphics("images/how_to_custom_gpt.png")
```
:::

::: {.column width="14%"}
2)  

```{r}
knitr::include_graphics("images/how_to_custom_gpt2.png")
```
:::

::: {.column width="60%"}
```{r, out.height=400}
knitr::include_graphics("images/gpt_builder.png")
```
:::
::::

-   GPT Builder asks more questions
-   Try it out and keep "talking into it" until it fits

## Custom GPT variant: Digital Twins

-   Training a model with the knowledge of a person

-   Others can communicate with the twin without interrupting the person at work

-   My experiment: upload all my papers/work: Digital Twin with my knowledge

## Tools based on LLMs

-   [**Perplexity AI**](https://www.perplexity.ai/): like search engine, scientific references
    -   [Example testosterone & behaviour](https://www.perplexity.ai/search/what-are-the-2WzaWWlKRm.4ge6IDNvx3w)
-   [**Elicit AI**](https://elicit.com/): Analyse research papers at superhuman speed
    -   Slides [here](https://hannahmetzler.eu/digital_tools_research/04_literature/index.html#7)
    -   [Beispiel](https://elicit.com/notebook/842fc893-e8f5-49a9-8bfb-d2ec6d1831fb)
- NotebookLM: Create podcasts using papers

# Ethics

## Data

## Copyright

## Significance for the future

-   The role of AI technologies such as ChatGPT will continue to grow
-   Nature of many jobs will change
-   People who can deal with it creatively and critically are in demand
-   There are no "experts" yet, a "right use"
-   We are constantly learning how best to use them.
- Keep using & experimenting

## Resources

- 2 [Erklär mir die Welt](https://xn--erklrmir-3za.at/) Podcasts with Malcolm Werchota:
  - [#283 Erklär mir ChatGPT](https://xn--erklrmir-3za.at/2023/12/27/283-erklaer-mir-chatgpt-malcolm-werchota/)
  - [#313 deep dive: Alle eure Fragen zu K.I., beantwortet](https://xn--erklrmir-3za.at/2024/07/30/deep-dive-alle-eure-fragen-zu-k-i-beantwortet/)

### Staying up to date

:::: columns
::: column
- Ethan Mollick
  - Newsletter: [One useful thing](https://www.oneusefulthing.org/)
  - On [LinkedIn](https://www.linkedin.com/in/emollick/)
  
:::
::: column
- [Nate Jones on Tiktok](https://www.tiktok.com/@nate.b.jones)
- Twitter ???

:::
::::

## Resources 2

:::: columns
::: column
- Youtube for technical understanding ["3Blue1Brown"](https://www.youtube.com/watch?v=wjZofJX0v4M)

:::
::::

## Deep Dives & Big Picture

- [Co-Intelligence: Living and Working with AI ](https://www.amazon.de/Co-Intelligence-Living-Working-Ethan-Mollick/dp/0753560771) by Ethan Mollick
- Read papers


::: notes
Joao talk

Evaluating LLMs - what is trustworthy - [Chatbot Arena](https://chat.lmsys.org/) - Leaderbord - r/LocalLamma comments: best place to learn about this stuff - a Starter guide for playing with your own local AI - The llama hitchiking guide to local LLMs Other good sources: Medium, Twitter

Mixture of experts - very promising: another architecture, e.g. Mixtral: trained with 8 mistrals, at each token it is chosing 2 different experts, trained on high dimensional separations of the data

Best open source model: Mixtral LoRa: Low Rank Adaptation: https://arxiv.org/abs/2106.09685 tps: tokens per second
::: -->
