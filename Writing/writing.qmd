---
title: "Using Chatbots for Writing"
subtitle: "Mastering AI Chatbots: Optimizing Research through Effective Use"
author: "Konstantin Hebenstreit"
date: "October 29 2024"
format: 
  revealjs: 
    transition: "slide"
    ## Defines the theme of the presentation, both version and style
    theme: [default, custom.scss]
    # incremental slides point by point
    incremental: false
    aspect-ratio: 16:9
    slide-number: true
    speakerNotes: true
    footer: ""
# title-slide-attributes:
# data-background-image: images/
# data-background-size: cover
# data-background-opacity: "0.5"
#editor
editor: source
---

## Writing sections of papers

### Test how good is the AI in writing a discussion
Try out: (Re-)Writing the discussion of a given paper

1. Choose a paper you know very well, best would be one of your own.
2. Give the paper without the discussion to the model "o1-preview"
3. Let it write the discussion on it's own
4. Check how it is doing

## My example
### Paper about generalization of prompting strategies

![](images/peerj_screenshot.png)

:::footer
Hebenstreit et al. [2024](https://peerj.com/articles/cs-1999/#)
:::

## AI generated discussion:{.smaller style="font-size: 0.4em;"}
In this study, we conducted a comprehensive comparison of zero-shot chain-of-thought (CoT) reasoning strategies across multiple large language models (LLMs) and diverse datasets. Our findings highlight that while the effectiveness of specific reasoning strategies can vary depending on the model and dataset, overall gains from CoT reasoning remain robust. Notably, GPT-4 demonstrated the most significant improvements when applying advanced reasoning prompts, particularly the “Zhou” prompt discovered through automated prompt engineering (Zhou et al., 2023b).

Effectiveness of Reasoning Strategies

Our results indicate that employing explicit reasoning prompts generally outperforms direct prompting without any specific CoT trigger. The “Zhou” prompt—“Let’s work this out in a step by step way to be sure we have the right answer.”—consistently yielded the highest performance across models, especially with GPT-4. This aligns with previous research suggesting that guiding models to think step by step enhances their ability to solve complex problems (Wei et al., 2022; Kojima et al., 2022).

Interestingly, while GPT-4 benefited significantly from externalized reasoning prompts, models like Flan-T5-xxl did not exhibit the same level of improvement. For Flan-T5-xxl, direct prompting remained among the optimal methods. This suggests that the efficacy of CoT reasoning strategies may be model-dependent, possibly due to differences in training data, architecture, or inherent reasoning capabilities.

Model Performance Across Datasets

The performance of models varied notably across different datasets. GPT-4 and GPT-3.5-turbo achieved the highest scores overall, particularly excelling in datasets like WorldTree v2 and CommonsenseQA. These datasets, which require general knowledge and logical reasoning, appear to be well-suited for advanced LLMs. The superior performance on WorldTree v2 suggests that better models find this dataset increasingly easier, potentially due to their enhanced ability to process and reason with scientific facts.

Conversely, StrategyQA presented challenges, with models showing lower performance due to the dataset’s ambiguous items. This highlights the impact of question clarity on model performance and suggests that even advanced models struggle with poorly defined problems.

The medical datasets, MedQA and MedMCQA, were the most difficult for all models. Only GPT-4 and GPT-3.5-turbo displayed acceptable performance levels, underscoring the complexity of medical knowledge and the limitations of LLMs without specialized training in this domain. The low performance of models like Flan-T5-xxl on MedQA (Krippendorff’s alpha close to chance) emphasizes the need for domain-specific fine-tuning to improve accuracy in specialized fields.

Impact of Data Contamination

(...)

:::notes
People often tell me that they are frustrated because they have to read all of that text the AI is providing.

We need to find a method that is easy for us. Easy to test if the Chatbot is doing what we want.
:::

## Don't let the AI work against you!

- Possible strategy, but usually the output is not good
- Too much text to check
- Lot's of correction work for you

. . .

### $\rightarrow$ Next (smarter) try

Model input - to do ?????????????????????????

## Different strategy: bullet points{.smaller style="font-size: 0.4em;"}
- **Effectiveness of Chain-of-Thought (CoT) Reasoning Strategies:**
  - Applying CoT reasoning strategies consistently outperforms direct prompting across various models and datasets.
  - Reinforces the value of externalized reasoning in enhancing LLM performance on complex question-answering tasks.

- **Model-Specific Benefits:**
  - **GPT-4:**
    - Exhibited the most significant improvement with CoT prompts, particularly with the prompt discovered by Zhou et al. (2023b).
    - Demonstrated superior performance across most datasets, especially in domain-specific ones like medical datasets.
  - **Command-XL:**
    - Showed notable gains from externalized reasoning, similar to GPT-4.
  - **FLAN-T5:**
    - Performance did not significantly improve with CoT prompts; direct prompting remained among its optimal methods.
    - Possible data contamination due to fine-tuning on similar datasets may have influenced results.

- **Prompt Variability and Dataset Interaction:**
  - Different prompts yielded varying effectiveness depending on the dataset.
    - **"Zhou" Prompt:**
      - Excelled in datasets like OpenBookQA but was less effective in StrategyQA.
    - **"Rephrase" Prompt:**
      - Improved performance on StrategyQA, suggesting benefits in handling ambiguous questions through rephrasing.
  - Indicates the importance of aligning prompt design with dataset characteristics for optimal performance.

- **Performance on Domain-Specific Datasets:**
  - **Medical Datasets (MedQA and MedMCQA):**

**(... 2 times this amount of text)**

## How are we doing this time?

- Still too much information for a quick check
- What we want: 
  - Structure of a scientific discussion
  - Most important points for each section
  
. . .

### $\rightarrow$ Next (even smarter) try

Model input - to do ?????????????????????????

## Fit it into subheadings{.smaller style="font-size: 0.5em;"}


Conclusion

	•	Zero-shot chain-of-thought (CoT) reasoning strategies consistently enhance the performance of large language models (LLMs) across diverse datasets and domains.
	•	GPT-4 showed the most significant improvements when applying state-of-the-art reasoning prompts, outperforming other models in both general and specialized tasks.
	•	The effectiveness of CoT prompts remains robust across different models and datasets, highlighting their generalizability.
	•	Prompts discovered through automated methods, such as the one by Zhou et al., demonstrated superior performance, especially with GPT-4.
	•	Data contamination influenced the performance of models like Flan-T5-xxl, underscoring the importance of clean evaluation datasets.

Limitations

	•	The study was limited to six models and six datasets, which may not represent the full spectrum of available LLMs and task complexities.
	•	Data contamination in models like Flan-T5-xxl may have skewed results, particularly on datasets it was fine-tuned on.
	•	The evaluation metric, Krippendorff’s alpha, while suitable for varying answer choices, might not capture all nuances of model performance.
	•	The use of a zero temperature setting for determinism does not reflect real-world applications where randomness is a factor.
	•	Sample sizes, although determined via power analysis, might still limit the generalizability of the findings.

Related Work

	•	Previous research has demonstrated that CoT reasoning enhances LLM performance in few-shot prompting scenarios (Wei et al., 2022).
	•	Kojima et al. (2022) introduced zero-shot CoT prompting, showing improvements without task-specific examples.
	•	Zhou et al. (2023b) utilized automated prompt engineering to discover effective reasoning prompts, aligning with our findings.
	•	Studies on data contamination (Carlini et al., 2021) highlight its impact on model evaluations, resonating with our observations on Flan-T5-xxl.
	•	The robustness of CoT strategies across models and tasks adds to the growing evidence of their effectiveness in NLP tasks.

Future Work

	•	Expand the study to include a broader range of LLMs, including open-source and differently architected models.
	•	Investigate the combined effects of few-shot and one-shot prompting strategies with CoT reasoning.
	•	Develop methods to mitigate data contamination to ensure fair and accurate model comparisons.
	•	Explore new reasoning prompts through automated and human-in-the-loop approaches to further enhance performance.
	•	Evaluate CoT strategies in additional domains like legal or technical fields to test their generalizability.
	•	Examine the impact of varying model parameters, such as temperature settings, on the effectiveness of CoT reasoning.
	
## If you want to compare to the original{.smaller style="font-size: 0.35em;"}

**Using the chatbot to turn the original discussion into bullet points** 

### Conclusion

- Using reasoning strategies enhances model performance over direct prompting for all model sizes.
- Zhou and Kojima's prompts are effective across diverse scenarios.
- Comparing prompts is challenging because large models max out on simple data, and small models struggle on complex data.
- GPT-4 consistently outperforms other models, and Krippendorff's alpha effectively evaluates multiple-choice datasets.

### Limitations

- Dataset subsampling due to budget limits restricts comparisons with full test sets.
- Ambiguous questions in some datasets without clear best answers affected the evaluation.
- Evolving closed-source models hinder replication and may have data contamination issues.
- Data leakage from retraining on usage data is a concern, advising caution when interpreting results.

### Related Work

- Earlier studies assessed zero-shot prompting on medical and commonsense datasets.
- Research explored enhancing zero-shot reasoning via sequential steps and logic dependencies.
- This study focuses on simple chain-of-thought prompts across models and complex datasets.

### Future Work

- Evaluate prompts and datasets with more models, including open LLMs like LLaMa and Alpaca.
- Investigate effects of semantically similar but syntactically different prompts on performance.
- Conduct user studies to assess the quality and explanatory utility of reasoning chains from different prompts and models.

## Test how it is doing by comparing

<br>
<br>

### Do a side by side view if it captures the main points

## In real life it probably needs refinement

In a paper where you need to write the discussion, you can just copy the suggested discussion in bullet point format into a Google docs and work on it. 

<br>

Using the capabilities of the Chatbot for you not against you...

- Your work starts here
- Thinking about text & argument flow
- Update bullet points

## Let LLM draft the discussion

- When you are happy with it, then it is time to let the Chatbot write the whole text.
- This is what the model does well

## Final step: Writing the paragraphs

- Have the chatbot summarize the original discussion of the paper in bullet points of the subheadings.

- Have the chatbot create the discussion section with bullet points.

- Tip: One paragraph at a time

## Final test
Give the bullet points of the original discussion to the Chatbot and let it write

## Take aways for scientific writing with LLM Chatbots

1. Let the Chatbot create a draft of your discussion in bullet points. 
1. Refine the structure & overview manually.
1. Then let the Chatbot write:
    - one paragraph
    - refine manually
    - provide existing paragraphs as context for the next

## Writing: Use cases for academics

- Ideas where you could use LLM chatbots to write?

. . .

- Revise, proof-read, summarise, shorten, finding titles, review
- Refine your own writing (Book example Tim Skern)
- Tone/style checking

## Concrete examples we have done

- Getting feedback on specified aspects of drafts [Example](https://www.clearerthinking.org/post/writing)
-   Providing your paper, the [reviewer requests](https://chatgpt.com/share/671669a4-77ac-800b-a49a-7d210914c464) for rewriting, let it rephrase
-   [Feeding it with my papers](https://chatgpt.com/share/67166dd7-d494-800b-a652-1f0bb114d354), digital scientific twin
-   [Recommendation letter](https://chatgpt.com/share/67166d34-1fa8-800b-ba58-0bc6da8dea1b)
-   Refining [cover letter & teaching statement](https://chatgpt.com/share/67166cc1-a2bc-800b-ad9f-28eb8b5078d9)

## Preparation: Setting the right tone

For scientific writing, the model is often not precise and concise enough. Guide it to have a specific writing style by giving an instruction or example texts.

## Writing style instructions {.scrollable .smaller}

Example of an Writing Style Guide:

### 1. Structure and Formatting
- Use simple, clear language focused on one main idea per sentence
- Employ short paragraphs (2-4 sentences) to enhance readability
- Begin with a concise, attention-grabbing title and subtitle
- Use headings and subheadings to organize content logically
- Incorporate formatting elements (e.g., bold text, italics) sparingly for emphasis
- Use bullet points or numbered lists for key takeaways or steps

### 2. Content Organization
- Start with the most important information (inverted pyramid style)
- Ensure each paragraph focuses on a single main idea
- Organize content into clear sections, each building upon the previous one
- Include occasional single-sentence paragraphs for impact

### 3. Flow and Engagement
- Open with a compelling hook to capture reader interest
- Pose thought-provoking questions to involve the reader
- Maintain a strong flow of ideas throughout the text
- Use transitional phrases to connect ideas and paragraphs
- Create and use strong "concept handles" (catchy phrases that sum up complex ideas)
- Use analogies and metaphors sparingly to explain complex concepts
- Incorporate relevant real or hypothetical scenarios to illustrate points

### 4. Depth and Conciseness
- Provide depth on key points without becoming verbose
- Break down complex ideas into digestible chunks
- Use precise language to convey maximum information in minimal words
- Begin with concrete examples before introducing abstract principles
- Use multiple, diverse examples to illustrate complex points
- Apply introduced ideas to real-world scenarios to demonstrate their relevance

### 5. Persuasion and Balance
- Present a balanced view by acknowledging multiple perspectives
- Address potential counterarguments proactively
- Use appropriate language to connect with your target audience

### 6. Technical and Scientific Writing
- Explain technical concepts in accessible language
- Provide context for why the topic matters to the reader or the broader field

### 7. Tone and Voice
- Employ active voice and direct language
- Maintain a professional yet accessible tone
- Avoid being overly academic or jargon-heavy
- Use an engaging and slightly conversational style to create a sense of dialogue
- Present ideas thoughtfully, showing respect for the reader's intelligence
- Convey complex ideas efficiently without sacrificing depth
- Maintain a forward-looking and slightly enthusiastic tone, especially when discussing potential developments
- Balance optimism about possibilities with acknowledgment of challenges
- Aim to inform and engage without being pedantic or oversimplifying
