---
format: 
  revealjs: 
    transition: "slide"
    ## Defines the theme of the presentation, both version and style
    theme: [default, custom.scss]
    # incremental slides point by point
    incremental: false
    aspect-ratio: 16:9
    slide-number: true
    speakerNotes: true
# title-slide-attributes:
# data-background-image: images/
# data-background-size: cover
# data-background-opacity: "0.5"
#editor
editor: source
---


# How do Large Language Models (LLMs) work?

**Mastering AI Chatbots: Optimizing Research through Effective Use**

<br>

**Mag. Dr. Hannah Metzler** <br>
*Complexity Science Hub &  Medical University of Vienna* <br>
4 December 2024

<small> Slides: https://hannahmetzler.eu/ai_skills </small>

## AI and Science

::::: columns
::: {.column width="47%"}
![](../Introduction/images/ai_scientist_news.png)
<small>*Source: [Discover Magazine](https://www.discovermagazine.com/the-sciences/meet-the-ai-scientist)*</small>
:::

::: {.column width="47%" .fragment}
![](../Introduction/images/comparing_numbers.png)

:::
:::::

## How does AI work?
<br>
<br>

::::: columns
::: {.column #vcenter width="20%" align="center"}
Input
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="25%" align="center"}
AI Model
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="20%" align="center"}
Output
:::
:::::

## AI
<br>
<br>

::::: columns
::: {.column #vcenter width="20%" align="center"}
![](https://images.pexels.com/photos/1414110/pexels-photo-1414110.jpeg)
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="25%" align="center"}
Image Classification Model
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="20%" align="center"}
Lemon
:::
:::::

## LLMs
<br>
<br>

::::: columns
::: {.column #vcenter width="20%" align="center"}
Text
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="25%" align="center"}
Large Language Model
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="20%" align="center"}
Text
:::
:::::


## LLMs
<br>
<br>

::::: columns
::: {.column #vcenter width="15%" align="center"}
Text
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="40%" align="center"}
![](../Introduction/images/tunable_parameters.png)
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="15%" align="center"}
Text
:::
:::::

## LLMs: Autocomplete
<br>
<br>

Context length

::::: columns
::: {.column #vcenter width="15%" align="center"}

<small>"The actress that played Rose in the 1997 film Titanic is named..."</small>


:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="40%" align="center"}
![](../Introduction/images/tunable_parameters.png)
:::
::: {.column #vcenter width="10%" align="center"}
→
:::
::: {.column #vcenter width="15%" align="center"}
_Predicts the next word_
:::
:::::

## Next word prediction

![](../Introduction/images/next_word_prediction.png)


## In technical words

-   Deep Learning: Multiple layers of "neurons"
-   Linear regressions calculate "weights" for the connections

::::: columns
::: {.column width="47%"}
-   Input layer: Quantitative representation of sentence
-   Output layer: One-hot encoding
:::

::: {.column width="47%"}
```{r, echo=FALSE}
knitr::include_graphics("../Introduction/images/neuralnet.svg")
```
:::
:::::

::: notes
One hot encoding: output has as many units as words, we predict the one with the maximum value, probabilities of words Input: one-hot times embedding, everything but the mask
:::

## Text representation
<br>
<br>

::::: columns
::: {.column width="40%" align="center"}
Words

<small>
Even though the sound of it is<br> something quite atrocious, if you<br> say it loud enough, you’ll always<br> sound precocious, <br>Supercalifragilisticexpialidocious!
</small>

<small>
*Marry Poppins*
</small>

:::

::: {.column width="10%" align="center".fragment data-fragment-index="1"}
→
:::

::: {.column width="40%" align="center" .fragment data-fragment-index="1"}
Tokens = "Subwords"

![](../Introduction/images/tokenzined_sentence.png)
:::

:::::

## Dictionary

The LLM has a dictonary of a specific number of tokens. 

In that way it does not have to know all words, but constructs the rare ones by combining some of its tokens.

Every single character is also one token, such that the LLM can construct all possible words from tokens.

## Text representation
<br>
<br>

::::: columns
::: {.column width="40%" align="center"}
Word/Token


<br>

<span
style="padding: 0.1em 0.em; background-color: #D7C9F3; border-radius: 4px;">Even
</span>
...

:::

::: {.column width="10%" align="center" .fragment data-fragment-index="1"}
→
:::
::: {.column width="40%" align="center" .fragment data-fragment-index="1"}
Word Embedding
<small>
[-0.3185, 0.5976, 0.4817, 0.7306, -0.5938, -0.6372, 0.9381, -0.9165, -0.9396, 0.3540,
0.0262, -0.6131, 0.3634, -0.0391, -0.4732, -0.2341, -0.8044, -0.3637, -0.5958, -0.8710,
0.3722, -0.8544, -0.7819, -0.5487, -0.9314, 0.3949, -0.3168, -0.3363, -0.6973, -0.3789,
0.7200, -0.6201, -0.7010, -0.3735, 0.7437, -0.9795, -0.4916, 0.2130, 0.6817, 0.1972,
0.8518, -0.8700, -0.4013, -0.6310, -0.9597, 0.2763, -0.9173, 0.2900, -0.1896, 0.8286,
-0.8617, 0.2566, 0.7024, -0.2448, 0.0994, -0.6664, -0.0699, -0.5830]
</small>
:::
:::::


## Representing the meaning of words

::::: columns
::: {.column width="40%"}
**Word embeddings**

![](../Introduction/images/wordembeddings.svg)

:::

::: {.column width="56%"}
**Contextual word embeddings**

-   Context (before and after the word) taken into account
-   [Example](https://www.tiktok.com/@whats_ai/video/7247609946672565510) "Apple": Mac or fruit?

<img src="../Introduction/images/apple_context.png" alt="Apple context" style="display: block; margin-left: auto; margin-right: auto; width: 250px;" />
:::
:::::

## What is an LLM, actually? 

- **Large Language Models (LLMs)**: Deep learning neural networks built to understand and generate text
- Trained on **vast amounts of text** from diverse sources (Internet, Wikipedia, social media, books, podcasts, etc.)
- Convert text data into **statistical patterns** to predict words and sentences


## How are LLMs trained?

**GPT**:
**G**enerative **P**re-trained **T**ransformer models (a model architecture for efficient computing):

- Generative: Produces/Generates Text
- Pre-trained: on large text corpus.
  - Autoregressive:
Always predicts the next word of a text.
Does not need any annotation.
- Transformer: type of the language model

## Comparing training routine to older models {.smaller}
Changes of how models are trained over time.
Earlier approaches for weaker models:

- train seperate models for different tasks, e.g. translation
- pre-train models on language, then fine-tune on task (e.g. BERT)

Todays approach for frontier models:

1. Pre-train on most of the available data from the internet via next-word prediction.
1. Fine-tune model on conversation setting and chain-of-thought reasoning (next slide).
1. Refining by receiving feedback from humans
1. In context learning: Explain the model a task in the context window. (Often no need for task specific fine-tuning)

## Chain of thought reasoning

Enhancing the models performance by increasing the number of output tokens. Reasoning happens while the models predicts each word. External reasoning.

<br>

![](../Introduction/images/chain_of_thought.png){width=20%}


::: footer
Kojima et al. [2023](https://arxiv.org/abs/2205.11916)
:::

## What is still missing?

![](../Introduction/images/unknown_caller.png)

. . . 

<br>

**Context information: prompting to give more context**

:::footer
[pixabay](https://pixabay.com/illustrations/keyboard-smartphone-unknown-call-6753584/)
:::



## RAG Retrieval Augmented Generation {.smaller}
The LLM is given a specific question.

An algorithm looks up a database for specific or up-to-date information that fits the topic of the question.

This information is automatically added into the context of the LLMs.

The LLM has very specific and reliable information to answer questiong.

Examples for databases: Scientific papers, Medical textbooks, internal files of companies.

## Current top models

-   **ChatGPT-4o and o1** (Open AI)
-   **Claude 3.5 Sonnet** (Anthropic)
-   **Gemini 1.5 pro** (Google)

best non-proprietary:
-   **Llama 3.1** (Meta)

## Strengths of different models and interfaces

-   [**ChatGPT-4o**](https://chat.openai.com/) (Open AI): web search, code execution, multimodal interactions, custom GPTs with document upload. (128,000 tokens)
-   [**Claude 3.5 Sonnet**](https://claude.ai/) (Anthropic): Writing style, beautiful text rendering, custom projects. (200.000 tokens)
-   [**Gemini 1.5 pro**](https://gemini.google.com/app) (Google): web search, code execution, multimodal interactions, integration with Google services, custom models. (up to 2 million tokens)


::: notes
multimodal interactions with ChatGPT described on next slide
:::


## Multimodal interactions {.smaller}

Most with ChatGPT and Gemini, some with Claude. Only Claude: Artifacts, formatting text and websites.

- File Uploads (& Claude)
- Browse
- Discovering and using custom GPTs (& Claude: projects)
- Upload & analyze images (& Claude)
- Create images with DALL-E
- Audio: Talking/Listening
- Data analysis (& Claude)
- Execute code

:::notes
add screenshots benefits gpt/claude
:::

## Chatbot Arena
Human ratings of chatbot responses
![](../Introduction/images/chatbot_arena.png)

<small>
Rank (UB): model's ranking compared to other models
Rank (StyleCtrl): model's ranking with style control (lenght, formatting, etc.)
</small>

:::footer
[Chatbot Arena](https://chat.lmsys.org/)
:::


## Different ChatGPT versions {.smaller}

- GPT4: more resource intensive = expensive
- GPT4o: optimized to be faster and cheaper
- GPT-4o mini: every day tasks
- o1-preview: thinks before it answers, for complex tasks (no web search)
  
  
$\rightarrow$ Konstantin's default: GPT4


Usage limits:
- GPT-4: 40 messages every 3 hours
- GPT-4o: 80 messages every 3 hours
- GPT-4o mini: no limit

- o1-preview: 50 messages per week 
- o1-mini: 50 messages a day 

::: footer
[ChatGPT blog on models](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini) [usage limits](https://help.openai.com/en/articles/6950777-what-is-chatgpt-plus#h_d78bb59065)

::: 


## Discussion 

- In groups of 3 (10 minutes): 
  - Which problems do you encounter when using LLMs? 
  - What works well?
- 5 minutes for sharing most important points with the whole group




## Appendix {visibility='hidden'}

### Access to models

| model      | free access                       | costs /month |
|------------|-----------------------------------|--------------|
| ChatGPT    | GPT-4o mini, GPT4-o limited, limits for multimodal   | 24€          |
| Claude   | 200 tokens/day                    | ca \$20      |
| Gemini      | 1 month trial of paid version, <br> high usage limits in free version                   | 22€     |
| Perplexity | 3 requests/day, 1000 tokens/month | \$17-20      |
| Elicit     | 1000 tokens/month                 | \$10-12      |
